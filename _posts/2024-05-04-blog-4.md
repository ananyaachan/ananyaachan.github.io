---
layout: post
title: Blog Post 4 - Detecting Fake News
---

This is a tutorial on using machine learning to detect fake news.

The data that we will use to build our model is from Kaggle:
https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset


We will use tensorflow to create neural networks in order to classify news as fake or not.

First, we need to import the necessary libraries.

```python
# for data manipulation
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

# for neural network
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

# for embedding visualization
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

## 1. Acquire Training Data
First we get the training data that is already hosted on a github url. This is the data from the Kaggle website after some cleaning.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url) #read data into a pandas dataframe
df.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

Each row of the dataframe represents a news article. The title column has the title of the article, the text column has all the text in the article and the fake column tells us if the article is fake or not (0 when article is true and 1 when article has fake news).


## 2. Make a Dataset

In order to use tensorflow and neural networks, we must convert our data frame into a tensorflow dataset with inputs and outputs.

```python
from nltk.corpus import stopwords


def make_dataset(df):
  """
  Input: dataframe with rows representing articles, with columns for article title, text and fake.
  Output: a tensorflow dataset with (title, text) columns as inputs and fake column as the output.
  """
  #remove stopwords
  stop = stopwords.words('english')
  df['title'] = df['title'].str.lower()
  df['text'] = df['text'].str.lower()
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  #create tf dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], 
            "text" : df[["text"]]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
  )
  #batch dataset to increase training speed
  return data.batch(100)
```

{::options parse_block_html="true" /}
<div class="got-help">
In my first draft, I did not convert all the text into lowercase before removing stopwords. One of my peers pointed out that if I don't do this then there could be different casings of stopwords that won't be removed. For example "aNd" would not be removed although it is a stopword. Hence, I updated my make_dataset function and added these two lines before removing the stopwords.
```python
df['title'] = df['title'].str.lower()
df['text'] = df['text'].str.lower()
```
I converted everything to lowercase since the stopwords in the NLTK corpus are all lowercase. So this made sure that all different casings of stopwords are removed. 
</div>
{::options parse_block_html="false" /}


{::options parse_block_html="true" /}
<div class="gave-help">
For the make_dataset function, I gave one of my peers a suggestion to remove the stopwords.
They had originally created a helper function to remove stopwords that looped over each sentence in the column and removed the words that are stop words. I suggested that they could use lambda functions in conjunction with the apply method to achieve this, without a loop. 

```python
df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
```

</div>
{::options parse_block_html="false" /}

Now we can call our function on our original dataframe.
```python
data = make_dataset(df)
```

Next, we need to split our data into training and validation. We use 80% for training and 20% for validation. We will import another dataset later for testing so we don't need to include the testing split here.

```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size)

len(train), len(val)
```




    (180, 45)


## 3. Create Models

We will use the keras functional API for constructing our model. 
We want to make three kinds of models:
1. Only using the article title as an input.
2. Only using the article text as an input.
3. Using both the article title and the article text as input.

We need to specify a keras.Input for each distinct predictor variable. Since we have two predictors, text and title, so we specify two keras.Input. We need to specify three parameters for each input:
- shape: describes a single data item/row. Since each article has only one title and text, the shape is (1, ) for each respective input.
- name: this describes the input tensor, so that we can use it later.
- dtype: defines the data type for each input tensor. Since both title and text have words, hence dtype will be string.

```python
# inputs

title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

Next, we need to do some data preprocessing. We create a function that takes in text data and standardizes it by:
- Making everything lower case
- Removing all punctuation

```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

Now we need to vectorize - this is necessary because our models cannot take in a column of strings as input. We need to somehow convert the text into a form that can be an acceptable input into training our models. Through vectorization we represent text as a vector (array, tensor). We will replace words by its frequency rank in the data. For example, the sentence "elephants love sugarcane" could be represented as [100, 33, 159], i.e. "elephants" is the 100 most common word in the data set, "love" is the 33rd most common word in the data set...

```python
vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

#adapt vectorize layer on title and text 
vectorize_layer.adapt(train.map(lambda x, y: x['title'] + x['text']))
```


## Function for modelling

Since we need to create three different models, we can use a function in order to avoid repetition of code.
Our functions takes in a list of inputs variable names. Then, we need to adapt the vectorization layers to the title and text each - through which the vectorization layer learns what words are common in the input (predictor). We also use a shared embedding layer for title and text together. Lastly, we add some hidden layers for both title and text features, inlcuding dropout layers which help avoid overfitting.

If the input list has more than 1 element, then we need to concatenate the features of each input. Then we pass the consolidated set of computed features some Dense layers. The last Dense layer should have a number of outputs equal to the number of classes in the data. So since our labels are either fake or not i.e. 1 or 0, thus we have 2 outputs for the last Dense layer. 

After this we specify the inputs and outputs that our keras.Model will have. In order to understand what each model is doing, we create a figure using keras.utils.plot_model which we return later. Then we compile the model by specifying the optimizer, loss function and metrics that we want to be noted as our model will be trained. Finally, we train our model by calling fit, we use the validation data that we had split earlier in order to run this training. We use 50 epochs for the training. We return the "history" of the training, so that we can see how the model performed as it was training, and we also return the model itself so that we can call it later on when we need to test it on our testing data.

```python
def model(predictors):
  """
  The function trains the model using the functional API based on our inputs to the model.

  Inputs: a list of inputs
  Output: a plot representing the model, history of the model training, and the model
  """

  #shared layer of embedding for text and title
  shared_embedding = layers.Embedding(size_vocabulary, 10, name = "embedding")

  # layers for processing the title
  vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
  title_features = vectorize_layer(title_input)
  title_features = shared_embedding(title_features)
  title_features = layers.Dropout(0.2)(title_features)
  title_features = layers.GlobalAveragePooling1D()(title_features)
  title_features = layers.Dropout(0.2)(title_features)
  title_features = layers.Dense(32, activation='relu')(title_features)

  # layers for processing the text
  vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
  text_features = vectorize_layer(text_input)
  text_features = shared_embedding(text_features)
  text_features = layers.Dropout(0.2)(text_features)
  text_features = layers.GlobalAveragePooling1D()(text_features)
  text_features = layers.Dropout(0.2)(text_features)
  text_features = layers.Dense(32, activation='relu')(text_features)

  if len(predictors) > 1:
    main = layers.concatenate([title_features, text_features], axis = 1)
    inputs = [title_input, text_input]

  elif predictors[0] == "title":
    main = title_features
    inputs = [title_input]

  else:
    main = text_features
    inputs = [text_input]
  
  main = layers.Dense(32, activation='relu')(main)
  output = layers.Dense(2, name = "fake")(main)

  model = keras.Model(
    inputs = inputs,
    outputs = output
  )
  fig = keras.utils.plot_model(model)
  model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
  )
  history = model.fit(train, 
                      validation_data=val,
                      epochs = 50, 
                      verbose = False)
  return fig, history, model
```

{::options parse_block_html="true" /}
<div class="got-help">
In my first draft, I created the features as global variables outside the function above. One of my peers pointed out to me that this does not necessarily the 'different' models. Since the same global text features and title features are used throughout all models, we are not clearing the 'memory'. This gives us inaccuracy when evaluating the model performance. Hence, I added the creation of features as variables within the function so that everytime the function is called, the features are created 'fresh' and were not already in memory.

</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="gave-help">
One of my peers repeated various lines of code several times for each different model. I suggested the use of functions in order to reduce code repetition. This makes it easier for a reader to achieve the task consizely. 

</div>
{::options parse_block_html="false" /}

Next, we can start using our function for our three desired models. 

##  Using only the article title

First, let's see how we do if we only use the title of the article as the input.

```python
from matplotlib import pyplot as plt
fig_1, history_1, model_1 = model(["title"])
fig_1
```
![blog3_output_19_0.png](/images/blog3_output_19_0.png)

```python
plt.plot(history_1.history["accuracy"], label = "accuracy")
plt.plot(history_1.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```


![blog3_output_18_2.png](/images/blog3_output_18_2.png)

We can see that the accuracy is fairly good, and both accuracy and validation accuracy have similar plots.

## Using only the article text


```python
fig_2, history_2, model_2 = model(["text"])
fig_2
```
![blog3_output_21_1.png](/images/blog3_output_21_1.png)

```python
plt.plot(history_2.history["accuracy"], label = "accuracy")
plt.plot(history_2.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```
![blog3_output_22_1.png](/images/blog3_output_22_1.png)

The accuracy here is higher at the begining when compared to the title only model. Moreover, it levels out at fewer number of epochs to a higher accuracy i.e. it reaches a greater accuracy faster.

## Using both the article title and text


```python
fig_3, history_3, model_3 = model(["title", "text"])
fig_3
```
![blog3_output_24_0.png](/images/blog3_output_24_0.png)

```python
plt.plot(history_3.history["accuracy"], label = "accuracy")
plt.plot(history_3.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```

![blog3_output_25_1.png](/images/blog3_output_25_1.png)

This model performs well too, similar to the text-only model.

## Comparing all three models


```python
plt.plot(history_1.history["val_accuracy"], label = "Title")
plt.plot(history_2.history["val_accuracy"], label = "Text")
plt.plot(history_3.history["val_accuracy"], label = "Title and Text")
plt.legend()
```

![blog3_output_27_1.png](/images/blog3_output_27_1.png)


Based on the three plots, we can see that the validation accuracy is high for fewer epochs for the "title only" and the "title and text" model. The title and text plot does seem to be a little above the title only plot so we can say that it is best to use both title and the text in order to predict if a news article is fake or not.

## 4. Model Evaluation

We can get the testing data hosted on the github url. After reading in the data, we need to fun our make_dataset function on it so that it is converted accordingly for being input into our model. 
```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
test_data = make_dataset(test_df)
```

Let's check how our model that has both title and text as inputs performs on the testing data.
```python
model_3.evaluate(test_data)
```

    225/225 [==============================] - 2s 10ms/step - loss: 0.1077 - accuracy: 0.9820





    [0.10767878592014313, 0.9820036292076111]



So if we used our model to detect fake news, we would be right 98% of the time, which is great!

## 5. Embedding Visualization

Let's explore the embedding that our model learned to see if there are any interesting patterns in the words that the model regarded as useful when differentiating real news and fake news. 

But first, what is a word embedding? A word embedding is a representation of a word in vector space. Each word is given a vector and the words that are related are closer to each other in vector space.

```python
weights = model_3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary() 
```


```python
weights.shape
```




    (2000, 10)



Since we created a 10-dimensional embedding when constructing our model, the weights is also 10-dimensional. But if we plot 10 dimensions, it will be really hard to understand the plot. So let's reduce the dimension to 2 by using principal component analysis (PCA).
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```

Let's make a dataframe so that we can easily use plotly to construct an interactive plot for the embedding.

```python
embedding_df = pd.DataFrame({
    'word': vocab,
    'x0'   : weights[:,0], #first dimension
    'x1'   : weights[:,1] #second dimension
})
embedding_df
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.021830</td>
      <td>0.002027</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.034029</td>
      <td>0.017725</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>-1.147259</td>
      <td>0.026845</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>0.369231</td>
      <td>0.002737</td>
    </tr>
    <tr>
      <th>4</th>
      <td>us</td>
      <td>-0.545151</td>
      <td>-0.064220</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>reilly</td>
      <td>0.921681</td>
      <td>-0.008307</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>himself</td>
      <td>1.305712</td>
      <td>-0.046493</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>widely</td>
      <td>-1.197376</td>
      <td>0.062967</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>threatening</td>
      <td>1.003623</td>
      <td>-0.027824</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>yesterday</td>
      <td>2.090672</td>
      <td>-0.152887</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>




Let's plot using plotly!

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_data=["word"])

fig.show()
```

{% include embedding_plot.html %}

We can see that the embedding plot is "stretched out" in 2 directions, this is because there are two labels for our output variable, one corresponding to fake news and the other corresponding to news that is not fake. If we hover over the data points on the left, we can see that "obamas", "trumps" are closer together, which makes sense since they could be related to each other.
