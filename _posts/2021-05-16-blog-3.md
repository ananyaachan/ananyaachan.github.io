---
layout: post
title: Blog Post 3 - Detecting Fake News
---

This is a tutorial on using machine learning to detect fake news.

The data that we will use to build our model is from Kaggle:
https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset


We will use tensorflow to create neural networks in order to classify news as fake or not.

First, we need to import the necessary libraries.

```python
# for data manipulation
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

# for neural network
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

# for embedding visualization
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

## 1. Acquire Training Data
First we get the training data that is already hosted on a github url. This is the data from the Kaggle website after some cleaning.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url) #read data into a pandas dataframe
df.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

Each row of the dataframe represents a news article. The title column has the title of the article, the text column has all the text in the article and the fake column tells us if the article is fake or not (0 when article is true and 1 when article has fake news).


## 2. Make a Dataset

In order to use tensorflow and neural networks, we must convert our data frame into a tensorflow dataset with inputs and outputs.

```python
from nltk.corpus import stopwords

"""
Input: dataframe with rows representing articles, with columns for article title, text and fake.
Output: a tensorflow dataset with (title, text) columns as inputs and fake column as the output.
"""
def make_dataset(df):
  #remove stopwords
  stop = stopwords.words('english')
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  #create tf dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], 
            "text" : df[["text"]]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
  )
  #batch dataset to increase training speed
  return data.batch(100)
```

Now we can call our function on our original dataframe.
```python
data = make_dataset(df)
```

Next, we need to split our data into training and validation. We use 80% for training and 20% for validation. We will import another dataset later for testing so we don't need to include the testing split here.

```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size)

len(train), len(val)
```




    (180, 45)


## 3. Create Models

We will use the keras functional API for constructing our model. 
We want to make three kinds of models:
1. Only using the article title as an input.
2. Only using the article text as an input.
3. Using both the article title and the article text as input.

We need to specify a keras.Input for each distinct predictor variable. Since we have two predictors, text and title, so we specify two keras.Input. We need to specify three parameters for each input:
- shape: describes a single data item/row. Since each article has only one title and text, the shape is (1, ) for each respective input.
- name: this describes the input tensor, so that we can use it later.
- dtype: defines the data type for each input tensor. Since both title and text have words, hence dtype will be string.

```python
# inputs

title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

Next, we need to do some data preprocessing. We create a function that takes in text data and standardizes it by:
- Making everything lower case
- Removing all punctuation

```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

Now we need to vectorize - this is necessary because our models cannot take in a column of strings as input. We need to somehow convert the text into a form that can be an acceptable input into training our models. Through vectorization we represent text as a vector (array, tensor). We will replace words by its frequency rank in the data. For example, the sentence "elephants love sugarcane" could be represented as [100, 33, 159], i.e. "elephants" is the 100 most common word in the data set, "love" is the 33rd most common word in the data set...

Since we have two columns of text data, we need two vectorizing layers, one for each, title and text. This is because the frequency rank needs to be relative to all the text in just that individual column. Hence we need two vectorize layers. Then, we need to adapt the vectorization layers to the title and text each - through which the vectorization layer learns what words are common in the respective input (predictor). We also use a shared embedding layer for title and text together. Lastly, we add some hidden layers for both title and text features, inlcuding dropout layers which help avoid overfitting.


```python
vectorize_layer1 = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

# layers for processing the title
vectorize_layer1.adapt(train.map(lambda x, y: x["title"]))
title_features = vectorize_layer1(title_input)

vectorize_layer2 = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

# layers for processing the text
vectorize_layer2.adapt(train.map(lambda x, y: x["text"]))
text_features = vectorize_layer2(text_input)

#shared layer of embedding for text and title
shared_embedding = layers.Embedding(size_vocabulary, 10, name = "embedding")
title_features = shared_embedding(title_features)
text_features = shared_embedding(text_features)

title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```

## Function for modelling

Since we need to create three different models, we can use a function in order to avoid repetition of code.
Our functions takes in a list of inputs variable names. If this list has more than 1 element, then we need to concatenate the features of each input. Then we pass the consolidated set of computed features some Dense layers. The last Dense layer should have a number of outputs equal to the number of classes in the data. So since our labels are either fake or not i.e. 1 or 0, thus we have 2 outputs for the last Dense layer. 

After this we specify the inputs and outputs that our keras.Model will have. In order to understand what each model is doing, we create a figure using keras.utils.plot_model which we return later. Then we compile the model by specifying the optimizer, loss function and metrics that we want to be noted as our model will be trained. Finally, we train our model by calling fit, we use the validation data that we had split earlier in order to run this training. We use 50 epochs for the training. We return the "history" of the training, so that we can see how the model performed as it was training, and we also return the model itself so that we can call it later on when we need to test it on our testing data.

```python
"""
The function trains the model using the functional API based on our inputs to the model.

Inputs: a list of inputs
Output: a plot representing the model, history of the model training, and the model
"""
def model(predictors):
  if len(predictors) > 1:
    main = layers.concatenate([title_features, text_features], axis = 1)
    inputs = [title_input, text_input]
  elif predictors[0] == "title":
    main = title_features
    inputs = [title_input]
  else:
    main = text_features
    inputs = [text_input]
  
  main = layers.Dense(32, activation='relu')(main)
  output = layers.Dense(2, name = "fake")(main) #2 outputs since two classes (0 and 1)

  model_ = keras.Model(
    inputs = inputs,
    outputs = output
  )
  fig = keras.utils.plot_model(model_)
  model_.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
  )
  history_ = model_.fit(train, 
                      validation_data=val,
                      epochs = 50, 
                      verbose = False)
  return fig, history_, model_
```

Next, we can start using our function for our three desired models. 

##  Using only the article title

First, let's see how we do if we only use the title of the article as the input.

```python
from matplotlib import pyplot as plt
fig_1, history_1, model_1 = model(["title"])
fig_1
plt.plot(history_1.history["accuracy"], label = "accuracy")
plt.plot(history_1.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```


![blog3_output_15_2.png](/images/blog3_output_15_2.png)

We can see that the accuracy is fairly good, well above 97%.

## Using only the article text


```python
fig_2, history_2, model_2 = model(["text"])
fig_2
plt.plot(history_2.history["accuracy"], label = "accuracy")
plt.plot(history_2.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```
![blog3_output_17_2.png](/images/blog3_output_17_2.png)

The accuracy here is lower at the begining when compared to the title only model. However, it does level out the same as the number of epochs increased.

## Using both the article title and text


```python
fig_3, history_3, model_3 = model(["title", "text"])
fig_3
plt.plot(history_3.history["accuracy"], label = "accuracy")
plt.plot(history_3.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```

![blog3_output_19_1.png](/images/blog3_output_19_1.png)

This model performs very well, in fact the starting accuracy is from 98%, higher than the title only model.

## Comparing all three models


```python
plt.plot(history_1.history["val_accuracy"], label = "Title")
plt.plot(history_2.history["val_accuracy"], label = "Text")
plt.plot(history_3.history["val_accuracy"], label = "Title and Text")
plt.legend()
```

![blog3_output_21_1.png](/images/blog3_output_21_1.png)


Based on the three plots, we can see that the validation accuracy is high for fewer epochs for just the "title only" and the "title and text" model. The title and text plot does seem to be above the title only plot so we can say that it is best to use both title and the text in order to predict if a news article is fake or not.

## 4. Model Evaluation

We can get the testing data hosted on the github url. After reading in the data, we need to fun our make_dataset function on it so that it is converted accordingly for being input into our model. 
```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
test_data = make_dataset(test_df)
```

Let's check how our model that has both title and text as inputs performs on the testing data.
```python
model_3.evaluate(test_data)
```

    225/225 [==============================] - 1s 4ms/step - loss: 0.1085 - accuracy: 0.9686





    [0.10845204442739487, 0.9686400294303894]



So if we used our model to detect fake news, we would be right 96.86% of the time, which is good!

## 5. Embedding Visualization

Let's explore the embedding that our model learned to see if there are any interesting patterns in the words that the model regarded as useful when differentiating real news and fake news. 

But first, what is a word embedding? A word embedding is a representation of a word in vector space. Each word is given a vector and the words that are related are closer to each other in vector space.

```python
weights = model_1.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab1 = vectorize_layer1.get_vocabulary()  #vocab for titles
vocab2 = vectorize_layer2.get_vocabulary()  #vocab for texts
```


```python
weights.shape
```




    (2000, 10)



Since we created a 10-dimensional embedding when constructing our model, the weights is also 10-dimensional. But if we plot 10 dimensions, it will be really hard to understand the plot. So let's reduce the dimension to 2 by using principal component analysis (PCA).
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```

Let's make a dataframe so that we can easily use plotly to construct an interactive plot for the embedding.

```python
embedding_df = pd.DataFrame({
    'title': vocab1,
    'text' : vocab2,
    'x0'   : weights[:,0], #first dimension
    'x1'   : weights[:,1] #second dimension
})
embedding_df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td></td>
      <td>0.318032</td>
      <td>-0.066757</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>[UNK]</td>
      <td>0.114563</td>
      <td>-0.158813</td>
    </tr>
    <tr>
      <th>2</th>
      <td>trump</td>
      <td>said</td>
      <td>2.892822</td>
      <td>3.607124</td>
    </tr>
    <tr>
      <th>3</th>
      <td>video</td>
      <td>trump</td>
      <td>2.719583</td>
      <td>-2.589701</td>
    </tr>
    <tr>
      <th>4</th>
      <td>to</td>
      <td>the</td>
      <td>7.254882</td>
      <td>-1.000709</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>costs</td>
      <td>recognize</td>
      <td>-1.569225</td>
      <td>-0.379942</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>convicted</td>
      <td>products</td>
      <td>2.136720</td>
      <td>0.946007</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>constitution</td>
      <td>have</td>
      <td>-1.455348</td>
      <td>-1.204003</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>closer</td>
      <td>coup</td>
      <td>0.343744</td>
      <td>3.221236</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>closed</td>
      <td>pennsylvania</td>
      <td>-0.678157</td>
      <td>0.245969</td>
    </tr>
  </tbody>
</table>
<p>2000 rows Ã— 4 columns</p>
</div>


Let's plot using plotly!

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_data=["title", "text"])

fig.show()
```

{% include embedding_plot.html %}

We can notice that the embedding is "stretched out" in 2 directions, one corresponding to fake news and the other corresponding to news that is not fake. If we hover over the data points, we can see that "attorney", "governors", "enforcement" are closer together, whch makes sense since they could be related to each other. Whereas, "obamas", "trump", "russians" seem to be on the other end, obviously these words are also related to each other, hence closer together in the vector space.