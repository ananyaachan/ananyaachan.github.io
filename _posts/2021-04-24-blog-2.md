---
layout: post
title: Blog Post 2 - Spectral Clustering
---

A tutorial on a simple version of the *spectral clustering* algorithm for clustering data points.

### Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (A@B). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (A@v). 

## Introduction

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering. 


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
![output_2_1.png](/images/output_2_1.png)


*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_4_1.png](/images/output_4_1.png)

### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![output_6_1_1.png](/images/output_6_1_1.png)


We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_8_1.png](/images/output_8_1.png)


Whoops! That's not right! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, we derive and implement spectral clustering. 

## Part A

## Prompt

Construct the *similarity matrix* $$\mathbf{A}$$. $$\mathbf{A}$$ should be a matrix (2d `np.ndarray`) with shape `(n, n)` (recall that `n` is the number of data points). 

When constructing the similarity matrix, use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). 

**The diagonal entries `A[i,i]` should all be equal to zero.** The function `np.fill_diagonal()` is a good way to set the values of the diagonal of a matrix.  

#### Note

It is possible to do this manually in a `for`-loop, by testing whether `(X[i] - X[j])**2 < epsilon**2` for each choice of `i` and `j`. This is not recommended! Instead, see if you can find a solution built into `sklearn`. Can you find a function that will compute all the pairwise distances and collect them into an appropriate matrix for you? 

For this part, use `epsilon = 0.4`. 

## Answer
We can calculate the pairwise distances of each point using pairwise_distances function from sklearn.metrics. This function outputs a matrix, which we label as A, representing the pairwise distance i.e. the `A[i,j]` entry is the distance between `X[i]` and `X[j]`. The distance between two points is calculated as the Euclidean distance by default. Specifically, our array X has two coordinates for each data point. To calculate the distance between two data points given for example that `X[i] = [1,2]` and `X[j] = [2,4]` then the Euclidean distance is: `sqrt(sum of (X[i] - X[j])^2)` i.e. `[1,2] - [2,4] = [-1,-2]` ->  `square of [-1,-2] = [1,4]` -> `sum of [1,4] = 5` -> so `sqrt(5)` is the euclidean distance value for our example.

However, we don't need the distance values in the matrix, we want to construct a similarity matrix given a particular epsilon. We compare our distance values in the matrix to the specified epsilon: 
- if `A[i,j] < epsilon: A[i,j] = 1`. This is because the distance falls within epsilon, hence true i.e. 1
- if `A[i,j] >= epsilon: A[i,j] = 0`. The distance is not within epsilon, hence false i.e. 0.
- The diagonal entries are set to 0 using the fill_diagonal function in numpy.


```python
from sklearn.metrics import pairwise_distances
epsilon = 0.4
A = pairwise_distances(X)
A[A < epsilon] = 1
A[(A >= epsilon) & (A!=1)] = 0
np.fill_diagonal(A, 0)
```


```python
from sklearn.metrics import pairwise_distances
epsilon = 0.4
A1 = pairwise_distances(X)
# use np.where:
# first parameter: A < epsilon, gives us the indexes that need to be updated
# the second parameter: the matrix that will be updated based on the indexes
# np.ones creates a matrix of 1s with dimensions len(X),len(X).
# the third parameter: the values at the indexes need to be updated to 0
# so we create a matrix of 1s and update the values to 0 at indexes for which A < 0.4
A1 = np.where(A1 < epsilon, np.ones((len(X),len(X))), 0)
np.fill_diagonal(A1, 0) #diagonal entries are set to 0
```

## Part B

## Prompt

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$, which is also called the *degree* of $$i$$. Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.  

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a "good" partition of the data when $$N_{\mathbf{A}}(C_0, C_1)$$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. Saying that this term should be small is the same as saying that points in $$C_0$$ shouldn't usually be very close to points in $$C_1$$. 

Write a function called `cut(A,y)` to compute the cut term. You can compute it by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 

It's ok if you use `for`-loops in this function -- we are going to see a more efficient view of this problem soon. 

## Answer

We saw in the prompt that
$$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
So the i values belong to cluster 0 and j values belong to cluster 1. So first we need to find the i and j values in cluster 0 and cluster 1 respectively using `np.where` then we loop over matrix A, in order to sum the entries based on how cut should be calculated.


```python
def cut(A,y):
    C1 = np.where(y == 1)[0] #get index for cluster 1
    C0 = np.where(y == 0)[0] #get index for cluster 0
    # loop over A, add the value at the entry to sum
    sum = 0
    for i in C0:
        for j in C1:
            sum += A[i,j]
    return sum
```

Compute the cut objective for the true clusters `y`. Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels. You should find that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


```python
cut(A,y)
```




    13.0




```python
v = np.random.randint(0, 2, size = n) #generate random labels
cut(A,v)
```




    1150.0



The cut objective is 13 for the true labels and 1150 for the random. These are different by a factor of more than 88. So we can see that the cut objective heavily favors the true clusters since a pair of clusters is good if the cut objective is small, since this makes the overall norm cut also small.

#### B.2 The Volume Term 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

Write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. For example, `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. Then, write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 

***Note***: No for-loops in this part. Each of these functions should be implemented in five lines or less. 


```python
def vols(A,y):
    C0 = np.where(y == 0)[0] #get indexes for cluster 0
    C1 = np.where(y == 1)[0] #get indexes for cluster 1
    #sum of entries over each row i.e. degree of each row as in prompt
    d = A.sum(axis = 1)
    return (sum(d[C0]), sum(d[C1]))

def normcut(A,y):
    v0, v1 = vols(A,y) #get the volumes of two clusters
    #use cut function to get cut objective and then use equation in prompt
    return cut(A,y)*( (1/v0) + (1/v1) )
```

Now, compare the `normcut` objective using both the true labels `y` and the fake labels you generated above. What do you observe about the normcut for the true labels when compared to the normcut for the fake labels? 


```python
normcut(A,y)
```




    0.011518412331615225




```python
normcut(A,v)
```




    1.0240023597759158



The normcut objective using the true labels is smaller than using the random labels. A smaller norm cut objective means a better partition of data points. Hence, the true labels are indeed better to use to cluster the points than the random ones.

## Part C
## Prompt

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $$A$$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Next, if you like linear algebra, you can show that 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

1. Write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 
2. Then, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. While you're here, also check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 

#### Programming Note

You can compute $$\mathbf{z}^T\mathbf{D}\mathbf{z}$$ as `z@D@z`, provided that you have constructed these objects correctly. 

#### Note

The equation above is exact, but computer arithmetic is not! `np.isclose(a,b)` is a good way to check if `a` is "close" to `b`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify. 

Also, still no for-loops. 

## Answer


```python
def transform(A,y):
    v0, v1 = vols(A,y) #get volumes for each cluster
    z = np.zeros(len(y)) #create a vector of zeros
    z[np.where(y == 0)[0]] = 1/v0 #for indexes of cluster 0, update z 
    z[np.where(y == 1)[0]] = -1/v1 #for indexes of cluster 1, update z 
    return z
```


```python
z = transform(A,y)
D = np.zeros((len(y), len(y))) #create a matrix of zeros
np.fill_diagonal(D, A.sum(axis = 1)) #update diagonal entries with the row sums of A
np.isclose((z@(D-A)@z)/(z@D@z),normcut(A,y))
```




    True



```python
# use isclose instead of == because computer arithmetic is not exact 
np.isclose(z@D@(np.ones(n)), 0)  
```




    True



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this for you. 

Use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_`. 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
z_ = minimize(orth_obj, z) #explicit optimization
z_min = z_['x'] #get the minimizing vector 
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Does it look like we came close to correctly clustering the data? 

## Answer


```python
#first cluster based on indexes from negative values in z_min
neg = X[np.where(z_min < -0.0015)[0]]
#second cluster based on indexes from positive values in z_min 
pos = X[np.where(z_min >= -0.0015)[0]] 
```


```python
plt.scatter(neg[:,0], neg[:,1])
plt.scatter(pos[:,0], pos[:,1], color = "red")
```
![output_41_1.png](/images/output_41_1.png)


Looks like spectral clustering indeed helped in correctly clustering the data!

## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color. How did we do? 

## Answer


```python
D_inv = np.linalg.inv(D)#compute the inverse of D
L = D_inv@(D-A) #construct the Laplacian matrix
```


```python
Lam, U = np.linalg.eig(L) #find the eigenvectors and eigenvalues
ix = Lam.argsort() #sort the eigenvalues
Lam, U = Lam[ix], U[:,ix]
# 2nd smallest eigenvalue and corresponding eigenvector
z_eig = U[:,1]
```


```python
neg_eig = X[np.where(z_eig < 0)[0]] #cluster 1 based on the sign of z_eig
pos_eig = X[np.where(z_eig >= 0)[0]] #cluster 2
plt.scatter(neg_eig[:,0], neg_eig[:,1])
plt.scatter(pos_eig[:,0], pos_eig[:,1], color = "red")
```
![output_47_1.png](/images/output_47_1.png)


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

Synthesize your results from the previous parts. In particular, write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. Demonstrate your function using the supplied data from the beginning of the problem. 

## Answer


```python
def spectral_clustering(X, epsilon):
    """
    A function which takes in input data X and distance threshold epsilon to 
    perform spectral clustering. 
    First, constructs the similarity matrix, and then the Laplacian matrix. 
    Computes the eigenvector with second-smallest eigenvalue of the
    Laplacian matrix.
    Returns an array of binary labels indicating the group that data point i
    is in, based on the eigenvector.
    
    X: array of input data points
    epsilon: distance threshold
    """
    #similarity matrix
    A = pairwise_distances(X)
    A = np.where(A < epsilon, np.ones((len(X),len(X))), 0)
    np.fill_diagonal(A, 0)

    #laplacian matrix
    D = np.zeros((len(X), len(X)))
    np.fill_diagonal(D, A.sum(axis = 1))
    L = (np.linalg.inv(D))@(D-A)
    
    #eigenvectors and eigenvalues
    Lam, U = np.linalg.eig(L)
    # sort and get eigenvector with 2nd smallest eigenvalue
    z_eig = U[:,Lam.argsort()][:,1]
    
    return np.where(z_eig >= 0, np.zeros(len(X)), 1)
```

Let's demonstrate this function using supplied data.


```python
#get labels using spectral_clustering
labels = spectral_clustering(X, epsilon = 0.6) 
#cluster 0
plt.scatter(X[np.where(labels == 0)][:,0], X[np.where(labels == 0)][:,1]) 
#cluster 1
plt.scatter(X[np.where(labels == 1)][:,0], X[np.where(labels == 1)][:,1], color = "red") 
```
![output_53_1.png](/images/output_53_1.png)


## Part H

Run a few experiments using your function, by generating different data sets using `make_moons`. What happens when you increase the `noise`? Does spectral clustering still find the two half-moon clusters? For these experiments, you may find it useful to increase `n` to `1000` or so -- we can do this now, because of our fast algorithm! 

## Answer

We make moons using:
- n = 200, noise = 0.1
- n = 1000, noise = 0.1
- n = 1000, noise = 0.2


```python
np.random.seed(1234)
n = 200
X1, y1 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X1[:,0], X1[:,1])
```
![output_57_1.png](/images/output_57_1.png)



```python
labels_1 = spectral_clustering(X1, 0.4)
plt.scatter(X1[np.where(labels_1 == 0)][:,0], X1[np.where(labels_1 == 0)][:,1])
plt.scatter(X1[np.where(labels_1 == 1)][:,0], X1[np.where(labels_1 == 1)][:,1], color = "red")
```
![output_58_1.png](/images/output_58_1.png)



```python
np.random.seed(1234)
n = 1000
X2, y2 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X2[:,0], X2[:,1])
```
![output_59_1.png](/images/output_59_1.png)



```python
labels_2 = spectral_clustering(X2, 0.4)
plt.scatter(X2[np.where(labels_2 == 0)][:,0], X2[np.where(labels_2 == 0)][:,1])
plt.scatter(X2[np.where(labels_2 == 1)][:,0], X2[np.where(labels_2 == 1)][:,1], color = "red")
```
![output_60_1.png](/images/output_60_1.png)



```python
np.random.seed(1234)
n = 1000
X3, y3 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X3[:,0], X3[:,1])
```
![output_61_1.png](/images/output_61_1.png)



```python
labels_3 = spectral_clustering(X3, 0.4)
plt.scatter(X3[np.where(labels_3 == 0)][:,0], X3[np.where(labels_3 == 0)][:,1])
plt.scatter(X3[np.where(labels_3 == 1)][:,0], X3[np.where(labels_3 == 1)][:,1], color = "red")
```
![output_62_1.png](/images/output_62_1.png)


We can see that as the noise increases, the data points become more spread out. Spectral clustering still appears to work as the noise increases although the clusters are less distinct.

## Part I

Now try your spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![output_65_1.png](/images/output_65_1.png)


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_67_1.png](/images/output_67_1.png)


Can your function successfully separate the two circles? Some experimentation here with the value of `epsilon` is likely to be required. Try values of `epsilon` between `0` and `1.0` and describe your findings. For roughly what values of `epsilon` are you able to correctly separate the two rings? 

## Answer


```python
#epsilon = 0.2
labels_4 = spectral_clustering(X, 0.2)
plt.scatter(X[np.where(labels_4 == 0)][:,0], X[np.where(labels_4 == 0)][:,1])
plt.scatter(X[np.where(labels_4 == 1)][:,0], X[np.where(labels_4 == 1)][:,1], color = "red")
```
![output_70_1.png](/images/output_70_1.png)


The spectral clustering correctly clusters the data points for epsilon = 0.2.


```python
#epsilon = 0.5
labels_5 = spectral_clustering(X, 0.5)
plt.scatter(X[np.where(labels_5 == 0)][:,0], X[np.where(labels_5 == 0)][:,1])
plt.scatter(X[np.where(labels_5 == 1)][:,0], X[np.where(labels_5 == 1)][:,1], color = "red")
```
![output_72_1.png](/images/output_72_1.png)


The spectral clustering still works correctly for epsilon = 0.5.


```python
#epsilon = 0.7
labels_6 = spectral_clustering(X, 0.7)
plt.scatter(X[np.where(labels_6 == 0)][:,0], X[np.where(labels_6 == 0)][:,1])
plt.scatter(X[np.where(labels_6 == 1)][:,0], X[np.where(labels_6 == 1)][:,1], color = "red")
```
![output_74_1.png](/images/output_74_1.png)


Looks like it doesn't work properly for epsilon = 0.7, but it was working for epsilon = 0.5. Let's check 0.6!


```python
#epsilon = 0.6
labels_7 = spectral_clustering(X, 0.6)
plt.scatter(X[np.where(labels_7 == 0)][:,0], X[np.where(labels_7 == 0)][:,1])
plt.scatter(X[np.where(labels_7 == 1)][:,0], X[np.where(labels_7 == 1)][:,1], color = "red")
```
![output_76_1.png](/images/output_76_1.png)


The spectral clustering doesn't correctly cluster the data points for epsilon = 0.6 either.    
Therefore, we can see that for values of epsilon <= 0.5, we can correctly separate the two rings.
